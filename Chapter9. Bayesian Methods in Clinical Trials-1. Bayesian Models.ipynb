{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad10aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated  Sun Oct 16 20:33:03 2022"
     ]
    }
   ],
   "source": [
    "cat('Last updated ', date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea77b87",
   "metadata": {},
   "source": [
    "# Clinical Trial Data analysis using R and SAS\n",
    "### Author : Ding-Geng (Din) Chen , Karl E. Peace, Pinggao Zhang\n",
    "\n",
    "* Note : This note book is created with R in Jupyter Notebook.\n",
    "* Note : I made this note book while studying the book. Additional R code in addition to the book is added if necessary such that a code is not provided or addntional explanation is needed.  For detail explanation, refer the book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b778d6",
   "metadata": {},
   "source": [
    "### 9.1 Bayesian Models\n",
    "#### 9.1.1 Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da49223",
   "metadata": {},
   "source": [
    "The most elemental beginning of a discussion of Bayesian methods is __Bayes' Theorem__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9fd67",
   "metadata": {},
   "source": [
    "__1) Bayes' Theorem__\n",
    "Bayes' Theorem relates the _P(H|E)_ to its inverse _P(E|H)_ for any two events E and H.\n",
    "\n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "P(H|E) & = & \\frac{P(H,E)}{P(E)} \\\\\n",
    "       & = & \\frac{P(E|H)P(H)}{P(E)}\\\\\n",
    "       & = & \\frac{P(E|H)P(H)}{P(E|H)P(H) + P(E|H^c)P(H^c)}\n",
    "\\end{eqnarray}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab834005",
   "metadata": {},
   "source": [
    "__2) Example__ <Br>\n",
    "Let <br>\n",
    "* $H$ : the event of a patieht having breast cancer\n",
    "* $E$ : the event of the patient having a positie mammogram.\n",
    "* $P(E|H)=0.95$, a mammogram is 95% accurate in detecting breaset cancer among patients with known breast cancer. (i.e., the __sensitivity__ of the mammogram)\n",
    "\n",
    "* $P(E^c|H^c)=0.99$, 99% accurate in failing to detect breasts cancer among patients not having breat cancer (i.e, the __specificity__ of the mammogram)\n",
    "    \n",
    "* $P(H)=0.01$ , 1% of subjects will have breast cancer. (i.e, the __prevalence__ of breast cancer P(H)=0.01)\n",
    "    \n",
    "    \n",
    "We can calculate $P(H|E)$, (i.e, the __precision__ of the mammogram)  of from (prior) knowledge of the information $P(H), P(E), P(E|H), P(E^c|H^c)$.\n",
    "    \n",
    "$$\\begin{eqnarray} \n",
    "P(H|E) & = & \\frac{P(E|H)P(H)}{P(E|H)P(H) + P(E|H^c)P(H^c)}\\\\\n",
    "       & = & \\frac{0.95 \\times 0.01}{0.95 \\times 0.01 + (1-0.99)\\times (1-0.01)} \\\\\n",
    "       & = & 0.49       \n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d5285b",
   "metadata": {},
   "source": [
    "We can also calculate $P(H^c|E^c)$ the probability that a patient does not have breaset cancer givein the mammogram was negarive (i.e., the __negative predictive probabiliy__ of the mammogram)\n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "P(H^c|E^c) & = & \\frac{P(E^c|H^c)P(H^c)}{P(E^c|H^c)P(H^c) + P(E^c|H)P(H)}\\\\\n",
    "       & = & \\frac{0.99 \\times (1-0.01)}{0.99 \\times (1-0.01) + (1-0.95)\\times 0.01} \\\\\n",
    "       & = & 0.999       \n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Whici is quite hith, particularly in comparison to the precision (0.49).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97396cfb",
   "metadata": {},
   "source": [
    "__3)prior, posterior distribution__ <br>\n",
    "Technically, Bayes' Theorem exporess the __posterior probability__ for a hypothesis H of having vreast cancer after a positive mammogram E is oberved - in terms of the prior probabioities of H and E, and P(E|H). \n",
    "\n",
    "We can rewrite $P(H|E)$ as \n",
    "\n",
    "$$P(H|E)  =  \\frac{P(E|H)P(H)}{P(E|H)P(H) + P(E|H^c)P(H^c)}$$\n",
    "\n",
    "$$\\downarrow$$\n",
    "\n",
    "$$ p(\\theta|y) = \\frac{L(\\theta)\\pi(\\theta)}{\\int L(\\theta)\\pi(\\theta)d\\theta} \\propto L(\\theta)\\pi(\\theta)$$\n",
    "\n",
    "where <br>\n",
    "\n",
    "* $E$ represents an observed data event $y$\n",
    "* $H$ is described in terms of a hypothetical parameter $\\theta$,\n",
    "* $P(E|H)$ is the __likelihood function__ $L(\\theta)=L(\\theta|y)$ and\n",
    "* $P(H)=\\pi(\\theta)$ is the __prior distribution__about the parameter  $\\theta$.\n",
    "\n",
    "* Note that the denominator of $P(H|E)$ is the total probability theorem so we can use integral in the continuous form.\n",
    "\n",
    "* $\\propto$ is the notation for \"proportional to\"\n",
    "* $p(\\theta|y)$ is the __posterior__ probabilitu distribuion in Bayesian statistics. \n",
    "* $\\int L(\\theta)\\pi(\\theta)d\\theta$ is a constant independent of the parameter $\\theta$ and is usually difficult to obtain especially for higher dimensional parameter vectors. This constant is sometimes called a __normalization constant__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc916f3",
   "metadata": {},
   "source": [
    "#### 9.1.2 Posterior Distribution for Some Standard distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca0f92",
   "metadata": {},
   "source": [
    "The next step is Bayesian modelling. In this section, the Posterior distributions for some standard distributions commonly used in Bayesian statistics and clinical trirals is introduced. \n",
    "<br>\n",
    "\n",
    "Two types of most commly used __prior distributions__ in Bayesian modelling. \n",
    "\n",
    "1) noninformative or reference prior: When no prior information is available about the parameter of interest. \n",
    "\n",
    "2) conjugate prior: A conjugate prior leds to the same distribution __family__ when combined with the likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cebf1",
   "metadata": {},
   "source": [
    "##### 9.1.2.1 Normal Distribution with Known Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58e4ba",
   "metadata": {},
   "source": [
    "Consider a set of i.i.d. observations $y_i$ with $i=1,2,\\cdots,n$ on a random variaable $Y$ and let $y=(y_1,\\cdots,y_n)$ represent all n obervations. Assume that $Y_i \\sim N(\\theta, \\sigma)$ with $\\sigma$ known.\n",
    "\n",
    "1) Noninformative or reference prior is $\\pi(\\theta)=1$, representing that no prior information is available. Then the posterior distribution is \n",
    "$p(\\theta|y) \\propto L(\\theta)\\pi(\\theta) = L(\\theta)=\n",
    "N(\\bar{y}, \\sigma^2/n)$\n",
    "\n",
    "$L(\\theta) = L(\\theta|y)$ Since y is i.i.d, $L(\\theta|y)=f(y|\\theta)$, $f(y)$ is normally distribution.\n",
    "\n",
    "2) The conjugate prior: The conjugate prior is a prior distribution when combined with the likelihood leads to the same distribution. <br> In the normal case, the conjugate prior is $\\pi(\\theta) = N(\\theta_0, \\tau^2)$ where $\\theta_0$ and $\\tau^2$ are the prior mean and variance, which leads to the posterior normal distribution.\n",
    "$$p(\\theta|y) \\propto N(\\mu_p, \\sigma^2_p)$$\n",
    "Where \n",
    "* The posterior mean is $\\mu_p  = \\omega\\bar{y} + (1-\\omega)\\theta_0 $ and\n",
    "* The posterior variance is $\\sigma^2_p= \\frac{1}{\\frac{n}{\\sigma^2}+ \\frac{1}{\\tau^2}}$\n",
    "\n",
    "The posterior variance $\\sigma^2_p$ incorporates variance from both the data and the prior distribution. And it is less than $\\sigma^2/n$, representing a shrinkage to the posterior mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3f788",
   "metadata": {},
   "source": [
    "##### 9.1.2.2 Normal Distribution with Known Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71443d0f",
   "metadata": {},
   "source": [
    "Now we assume that $Y_i \\sim N(\\theta, \\sigma)$ with $\\sigma$ unknown.\n",
    "\n",
    "1) the usual noninformative or reference prior is $\\pi(\\mu, \\sigma)\\propto 1/\\sigma$ \n",
    "\n",
    "The posterior distribution can be\n",
    "$p(\\mu,\\sigma|y) = p(\\sigma|y)p(\\mu|\\sigma, y)$ <br>\n",
    "\n",
    "where $\\sigma|y \\sim (n-1)s^2/\\chi^2_{n-1}$, which is the inverse-$\\chi^2$ distribution and $\\mu|\\sigma, y \\sim N(\\bar{y}, \\sigma^2/n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2360a7",
   "metadata": {},
   "source": [
    "##### 9.1.2.3 Normal Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a368cd",
   "metadata": {},
   "source": [
    "##### 9.1.2.4 Binomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecc88b",
   "metadata": {},
   "source": [
    "Now we assume that $y \\sim Binomial(n, \\theta)$ with $\\theta$ is tha binomial proportion parameter\n",
    "\n",
    "1)  The usual noninformative or reference prior is $\\pi(\\theta)=1$. This leads to the well-known beta-binomial as the posterior distribution. \n",
    "\n",
    "The posterior distribution ban ce\n",
    "$p(\\theta|y)= Beta(y+1, n-y+1)$ \n",
    "where\n",
    "$Beta(a,b)$ is a beta distribution with parameters a and b.\n",
    "$Bata(a,b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}x^{a-1}(1-x)^{b-1}$\n",
    "\n",
    "2) The conjugate prior Beta(a,b) as $\\pi(\\theta) \\propto \\theta^{a-1}(1-\\theta)^{b-1}$.\n",
    "\n",
    "The posterior distribution can be\n",
    "$p(\\theta|y)= Beta(a+y, b+n-y)$ \n",
    "\n",
    "This beta-binomial distribution is commnly used in clinical trials with binomial observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129cbd2a",
   "metadata": {},
   "source": [
    "##### 9.1.2.5 Multinomial Distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
